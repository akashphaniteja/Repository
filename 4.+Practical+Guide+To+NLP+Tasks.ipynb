{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "4. Practical Guide To NLP Tasks.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "W7hcEK_xJ2qy"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf9tiyWJpDe1",
        "colab_type": "text"
      },
      "source": [
        "# Walking Through an NLP Pipeline with SpaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3pNbSASwuXR",
        "colab_type": "text"
      },
      "source": [
        "Till now we've learnt quite a few things. We've learnt about preprocessing text, BOW models, topic models and word embeddings. In this notebook we'll be taking a more practical approach and learning how to deal with almost all the nlp problems that you'll be trying to solve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-js6XwD48EWD",
        "colab_type": "text"
      },
      "source": [
        "## Quick Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riegjqcz8MQ0",
        "colab_type": "text"
      },
      "source": [
        "We're going to be using a library called SpaCy later in this notebook. To make things easier we're quickly going to download an install the model now.\n",
        "\n",
        "If you're running on local run both steps one after another.\n",
        "\n",
        "For colab:\n",
        "\n",
        "All you need to do is run the following cell. Then, when it's finished running, restart the runtime above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVWLBmu88YPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python -m spacy download en_core_web_md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrGdE46_MhvL",
        "colab_type": "text"
      },
      "source": [
        "Then once you've done the above steps, run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hdv18vR1MtM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_md')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c88TX7-wuXU",
        "colab_type": "text"
      },
      "source": [
        "## Obtaining Data\n",
        "\n",
        "That's one of the most important and generally the most overlooked part. Real world data is hard. Converting it into a usable form is even harder. Generally you'd write your spiders to scrape content from web. A lot of the other times the data will be in pdf form and you'll convert it into txt. Here's a few libraries you can make use of:\n",
        "1. Web scraping: BeautifulSoup, scrapy\n",
        "2. Pdf to text: If you're on linux/macOS install `poppler-utils` and use pdf2text, for windows use `xpdf`. All other libraries in Python do not work that well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRkyTZ2t_Cbj",
        "colab_type": "text"
      },
      "source": [
        "### Web scraping with Beautiful Soup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO_un1OjA2Ya",
        "colab_type": "text"
      },
      "source": [
        "Beautiful Soup is an amazing powerful library that helps make scraping web pages easy and intuitive. We're not going to be using this for scraping annual reports, so we've just included a link quick walkthrough here. If you want to look at beautiful soup in more detail, there are loads of articles and courses dedicated to it e.g.: https://towardsdatascience.com/step-by-step-tutorial-web-scraping-wikipedia-with-beautifulsoup-48d7f2dfa52d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyOYAderI-Y1",
        "colab_type": "text"
      },
      "source": [
        "### Converting Pdfs to Text\n",
        "\n",
        "As mentioned earlier you can use pdf2text which is a command line utility. Can be used as: `pdftotext {PDF-file} {text-file}`\n",
        "\n",
        "You can find instructions to install here: https://www.cyberciti.biz/faq/converter-pdf-files-to-text-format-command/\n",
        "\n",
        "\n",
        "You can obtain xpdf for windows and use the same commands. Find xpdf binaries here: http://gnuwin32.sourceforge.net/packages/xpdf.htm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Tb8cs6JX8a",
        "colab_type": "text"
      },
      "source": [
        "#### **Exercise**: Download the PDF below, import it and print the first 50 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yexdVwfHJWXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pdf_url = [\"https://int.nyt.com/data/documenthelper/641-read-the-open-letter-to-amazon/0a112e77301026beb86d/optimized/full.pdf\"]\n",
        "\n",
        "# Import your processed pdf here\n",
        "# Print the first 50 characters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7hcEK_xJ2qy",
        "colab_type": "text"
      },
      "source": [
        "##### *Example Answer*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbodLGS6J6h2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'Open Letter to Amazon Chief Executive Jeff Bezos\\n\\n'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFsFdkgsOFo-",
        "colab_type": "text"
      },
      "source": [
        "Is your output the same?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3r6-hUnwuXW",
        "colab_type": "text"
      },
      "source": [
        "# Defining your problem\n",
        "\n",
        "Once you have the data, next we need to look at what the problem we're trying to solve is. \n",
        "\n",
        "*In fact, in a real life data science problem, this should be your first step. By really understanding what you are trying to achieve, you will be able to work far more efficiently on the problem. It's all well and good to go download all the data you can and start applying models to it, but without understanding what's important you won't be able to produce a good solution.*\n",
        "\n",
        "For a text problem your approach will fall into one of two categories:\n",
        "\n",
        "*   Using a supervised approach\n",
        "*   Using an unsupervised approach\n",
        "\n",
        "\n",
        "**Supervised Approach**\n",
        "This is the ideal approach when we're looking to predict something based on body of text we have, or can, label. It also requires us to have a large enough amount of text to train our own model on. \n",
        "\n",
        "If this is the case then you will simply convert your corpus into word vectors and train a model.\n",
        "\n",
        "**Unupervised Approach**\n",
        "In a lot of situations this isn't the case. More often than not we either wont have enough text for our model, or can't (realistically) label it. \n",
        "\n",
        "In this notebook we'll look at how you can solve problems of this type Obviously this is just the basic outline, but from here you can add more depth and detail as you become more confident. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YiRj8mBwuXX",
        "colab_type": "text"
      },
      "source": [
        "As a quick recap:\n",
        "1. Identify what problem it is that you're solving.\n",
        "2. If you can create supervised data -> create BoW representations and train anything from logistic regression to lightGBM over them\n",
        "3. If not, you'll need to create a nlp pipeline using some pre trained models\n",
        "\n",
        "Since supervised problems are more or less straightforward, we're going to focus on the unsupervised approach. \n",
        "\n",
        "For the rest of this notebook we're going to be working on an annual report for a company called Tullow Oil. We've already converted it to text, so you don't have to. The code to download the report is below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzDGsq1SwuXi",
        "colab_type": "text"
      },
      "source": [
        "#### About SpaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3nUM6EowuXj",
        "colab_type": "text"
      },
      "source": [
        "First introduced in the previous notebook, Spacy is an *Industrial-Strength Natural Language Processing Library*. It takes care of most of your underlying nlp tasks so we can focus on looking at the bigger picture. This allows us to spend more of our time trying to solve the (business) problem we face, rather than writing our own functions for common tasks.\n",
        "\n",
        "Some of the functionality Spacy has:\n",
        "*   Tokenisation\n",
        "*   Lemmatization\n",
        "*   Named Entity Recognition\n",
        "*  Part of Speech\n",
        "* NLP Models\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0tUN01xwuXl",
        "colab_type": "text"
      },
      "source": [
        "#### Downloading models\n",
        "\n",
        "\n",
        "The library itself doesn't come with models so you'll need to download them seprately. If you're working with Engligh language, there are mainly 3 of them: `en_core_web_sm`, `en_core_web_md`, `en_core_web_lg` in the increasing order of complexity: small, medium and large. You can install any of them using: `python -m spacy download en_core_web_xx`\n",
        "\n",
        "We don't need to do this now, as this was the setup step at the beginning of the article."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8pBahqkwuXm",
        "colab_type": "text"
      },
      "source": [
        "# Using Spacy Doc Objects\n",
        "\n",
        "Using the downloaded models you can convert your text into a spacy doc object. \n",
        "\n",
        "When your run the function `nlp()` a spacy doc object will be created from the given data.\n",
        "\n",
        "The `nlp()` function will run the spacy NLP pipeline and save all the metadata to the doc object. For example, the object will contain computed values for:\n",
        "\n",
        "*   Tokenisation\n",
        "*   Lemmatization\n",
        "* Part of Speech\n",
        "* Dependency Parsing\n",
        "* Named Entity Recogntition \n",
        "\n",
        "That's a lot to remember and explain at once, so we're going to work through each one with an example. The first two you should remember from the previous notebook. The last three we will cover below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDlCSrtpo5db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First, let's download that annual report we were talking about\n",
        "\n",
        "! wget https://auquan-public-data.s3.ap-south-1.amazonaws.com/Tullow_Oil.txt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbRie_EspN5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next we need to read the data\n",
        "\n",
        "with open('Tullow_Oil.txt', 'r', encoding='latin-1') as f:\n",
        "  data = f.read()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nQl-HPTtzTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now convert it into a SpaCy Doc\n",
        "doc = nlp(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHzEr_3nwuXu",
        "colab_type": "text"
      },
      "source": [
        "### **1. Breaking into sentences:**\n",
        "\n",
        "In most cases, it is better to deal with text formatted as sentences instead of the whole document at once.\n",
        "\n",
        "We've seen how to do this with NLTK and it's just as easy in SpaCy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHKiM9SiwuXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sents = [x for x in doc.sents]\n",
        "print(sents[:20])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STHsj470uKS8",
        "colab_type": "text"
      },
      "source": [
        "As you can see, there are a couple stray characters and other errors. SpaCy doesn't work perfectly but that's the case with any NLP algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbPHIy4FwuX3",
        "colab_type": "text"
      },
      "source": [
        "### **2. Getting all tokens and their root forms:**\n",
        "\n",
        "Again, we've seen this with NLTK and now we will do it with SpaCy.\n",
        "\n",
        "In SpaCy there is a single function that takes care of tokenization and lemmatization in one go."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es69mX5xwuX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for token in doc:\n",
        "    #print('token: %s, root: %s'%(token, token.lemma_))\n",
        "    if len(token.text.strip()) > 5:\n",
        "      print('token -> root: %s -> %s'%(token, token.lemma_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJsUZL8Jie4t",
        "colab_type": "text"
      },
      "source": [
        "#### **Exercise:** Split, tokenise and lemmatize this doc using SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZYSUz8Pi3hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget https://auquan-public-data.s3.ap-south-1.amazonaws.com/Facebook.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFciuRNIi4I_",
        "colab_type": "text"
      },
      "source": [
        "##### *Example Solution (look when finished)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss9-JVBEi76b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do exactly what we did for Tullow oil\n",
        "# Read the file\n",
        "# convert to spacy doc\n",
        "# Get all the tokens and their lemmas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC4yaWV0wuX9",
        "colab_type": "text"
      },
      "source": [
        "### **3. Get POS (Part Of Speech) tags for each of the tokens, like noun, verb, adverb etc.**\n",
        "\n",
        "Despite it's name, part of speech is an increadibly useful technique that classifies words at nouns, verbs, adverbs, adjectives etc. \n",
        "\n",
        "The significance of this might not be instantly apparent, but it is hugely powerful. By focusing on verbs we can understand whats happening in a sentence. Nouns can tell us what is doing something in the sentence. An advance usecase is to build a parse tree of the sentence(s) to understand their syntactic structure.\n",
        "\n",
        "In general, knowing the grammatical category of words is useful in downstream tasks as it helps in understanding the semantics behind the word. For example: `play` as a noun and a verb mean completely different things.\n",
        "\n",
        "Again, this is super easy to implement in Spacy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhdh1WfrwuX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for token in doc:\n",
        "    print('token: %s, POS: %s'%(token, token.pos_))\n",
        "\n",
        "#Warning: Long Output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtrpYgiql15f",
        "colab_type": "text"
      },
      "source": [
        "#### **Exercise**: Return the top 20 most frequently occuring nouns in the Facebook report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxslKu8gmXyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#First you'll need to do PoS on your doc\n",
        "\n",
        "#Next you'll have to create a frequency count of each word. What does this tell us?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjBN3eu0mTOS",
        "colab_type": "text"
      },
      "source": [
        "##### *Example Answer*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X136-1XmS_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def getCount(doc, k):\n",
        "  tokens = []\n",
        "  for token in doc:\n",
        "    if token.pos_ == 'NOUN':\n",
        "      tokens.append(token.text)\n",
        "  counter = Counter(tokens)\n",
        "  return counter.most_common(k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czYwDC65wuYC",
        "colab_type": "text"
      },
      "source": [
        "### **4. Get all noun chunks**\n",
        "\n",
        "Noun chunks can be thought of as a noun and the words that describe it. For example: 'the short man' would be a single noun chunk. These chunks help separate different nouns from each other, so a lot of times we are interested in the whole chunk as this gives us more info. \n",
        "\n",
        "Here's how you can obtain noun chunks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-67mwy7wuYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk)\n",
        "\n",
        "#Really long output!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rkbVBfJ_sutd"
      },
      "source": [
        "#### **Exercise**: Create a list of noun chunks for facebook...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DSjl1eL_sutg",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3DIElGOysutj"
      },
      "source": [
        "##### *Example Solution (look when finished)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XpTQ-Lmzsutk",
        "colab": {}
      },
      "source": [
        "# Use doc.noun_chunks for getting the noun chunks\n",
        "# filter out the ones that're too short or have punctuations etc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5Qdpat3wuaa",
        "colab_type": "text"
      },
      "source": [
        "### **5. Get all named entities**\n",
        "\n",
        "Another common and really useful task is finding all the named things in a text. Named entities might include: people, geographical locations, organizations, reference of money, reference of quantities etc. \n",
        "\n",
        "The Spacy doc contains the named entities and their classes, so you can call both (as shown below).\n",
        "\n",
        "Once again, Spacy has built-in support to achieve this in just a line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTb_5EXMwuaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we're printing the named entity and the type of that entity\n",
        "for ent in doc.ents:\n",
        "    print('Entity: %s, Label: %s'%(ent.text, ent.label_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iyS4Tkjwua_",
        "colab_type": "text"
      },
      "source": [
        "You can find more details here: https://spacy.io/usage/linguistic-features\n",
        "        \n",
        "Keep in mind that you always want to start with the small spacy model (`en_core_web_sm`). If this appears to work contains too many errors, then try the medium or large model. \n",
        "\n",
        "Using the smaller model allows you to experiment faster (as it takes less time to load and run). On a more conceptual level, it is generally true that if something can be explained with a simpler model, then that is preferable to a larger model. In finance this is especially true because overfitting is such a problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHPZUBwYwubA",
        "colab_type": "text"
      },
      "source": [
        "Now lets build on this a bit more.\n",
        "\n",
        "In the next example we will be using these bits and pieces to try and extract some more interesting information out of Annual report. \n",
        "\n",
        "To begin, let's try to get all the names of people:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0luzSszSvCYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "people = []\n",
        "for ent in doc.ents:\n",
        "    if (ent.label_ == 'PERSON') and len(ent.text) > 5:\n",
        "      print(ent.text)\n",
        "      people.append(ent.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMBsNIVGzSBN",
        "colab_type": "text"
      },
      "source": [
        "Next lets step it up again.\n",
        "\n",
        "Now let's try something non-trivial. Let's find who's the ceo of the firm.\n",
        "\n",
        "This is going to be a multi-step process:\n",
        "1. Break your text into sentences\n",
        "2. Look at each sentence and find if there are mentions of specific keywords, `CEO` and `Chief Executive Officer` in our case\n",
        "3. If there is, get all the names of people from the sentence\n",
        "4. If most names of the same person then that probably is the CEO\n",
        "\n",
        "\n",
        "Additional: to be more sure you can actually look at the how close to your keyword does the name occur. In this case, closer it is, the more chance of it being a CEO \n",
        "\n",
        "So, first let's begin by getting all the noun chunks that mention either ceo or chief executive officer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RDon93pqNOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# look at each sentence\n",
        "for sent in doc.sents:\n",
        "  # for each tag we're intereseted in\n",
        "  for tag in ['ceo', 'chief executive officer']:\n",
        "    # if the tag appears in the sentence\n",
        "    if tag in sent.text.lower():\n",
        "      # get all the named entities in the sentence\n",
        "      for ent in sent.ents:\n",
        "        # if the entity is a person, print it\n",
        "        if (ent.label_ == 'PERSON') and len(ent.text) > 5:\n",
        "          print(ent.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y1estu10avw",
        "colab_type": "text"
      },
      "source": [
        "Well, here we have it! Simple working strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAa3S_TdwqLV",
        "colab_type": "text"
      },
      "source": [
        "#### **Exercise**: Try to obtain the names of COO and CFO for Facebook using a similar strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxwUp1MEwxL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add the steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHlciN92wySx",
        "colab_type": "text"
      },
      "source": [
        "##### *Example Solution (look when finished)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTyISWRvwzAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the same strategy as you used for CEO, if it doesn't work see if you can add more keywords and make it work, \n",
        "# or add some other rule to find out sentences that might mention that title and person \n",
        "# Your answers should be:\n",
        "# COO: Sheryl Sandberg\n",
        "# CFO: David Wehner\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p6U0d_fxA_S",
        "colab_type": "text"
      },
      "source": [
        "### **Final Exercise**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUHqjJVp0wRS",
        "colab_type": "text"
      },
      "source": [
        "Now let's look at all the statements that talk about money. If we get all such sentences we can find out the ones that talk about the financials we're interested in. For example, we might find a sentence that talks about money and it also says `Adj EBITDAX` (and probably no other relevant noun chunk) then we can be pretty sure that the value is actually `Adj EBITDAX`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31RA0pXd0Zgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sent in doc.sents:\n",
        "  for ent in sent.ents:\n",
        "    if ent.label_ in ['MONEY']:\n",
        "      print(sent)\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnVaIppD36pj",
        "colab_type": "text"
      },
      "source": [
        "**Exercise: Can using the above sentences and whatever you've learnt so far, try to extract Revenue and Adjusted EBITDAX**\n",
        "\n",
        "Make sure that your approach is general enough to work out for reports from different companies. Don't worry if you're unable to make it work for all of them, it's a difficult problem with all the different formats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiNiCuZaqAnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbhn3cIjxHpG",
        "colab_type": "text"
      },
      "source": [
        "##### *Example Solution (look when finished)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgVCdCuXxKaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Revenue: $1723m \n",
        "# NET DEBT $3.5Bn"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}