{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "1. Bag of words models.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jjMbIGlS9-3Z",
        "PnxCspl7-eGb",
        "B2-J-sF9AFHs"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z8Bs18FzhKA",
        "colab_type": "text"
      },
      "source": [
        "### Representing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nQYt8YLzhKC",
        "colab_type": "text"
      },
      "source": [
        "As described in the introduction, in this notebook we'll be looking at how to represent text in a way that could be useful for doing analysis. \n",
        "\n",
        "\n",
        "In all programming languages, text is represented in the form of a string. However, when we do any statistical analysis we want numerical vectors. More than this, we need to give the text a numerical representation that maintains all of the information contained within the text - e.g. word relationships and contextual meaning. \n",
        "\n",
        "\n",
        "Below you will learn one method to convert text into numbers - bag of words models. This is a first step to creating numerical representations of the information encoded in the text. \n",
        "\n",
        "\n",
        "Let's find out how."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frgayUcbzhKE",
        "colab_type": "text"
      },
      "source": [
        "### Bag of words models (BoW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew83cRqtzhKF",
        "colab_type": "text"
      },
      "source": [
        "BoW is the most common way of representing text. In this approach we take a given string and represent it as a frequency count of all the words it contains. For example the string: *\"Sometimes I like to eat an orange, but sometimes I prefer eating an apple\"* Would be represented as:\n",
        "\n",
        "* Sometimes: 2\n",
        "* I : 2\n",
        "* Like: 1\n",
        "* To: 1\n",
        "* Eat: 1\n",
        "* An: 2\n",
        "* Orange: 1\n",
        "* But: 1\n",
        "* Prefer: 1\n",
        "* Eating: 1\n",
        "* Apple: 1\n",
        " \n",
        "   \n",
        "Whilst you're working through implementing this approach, think about what might be a drawback of this approach.\n",
        "\n",
        "The method to building a BoW model has two main parts:\n",
        "1. Keep account of all the words/tokens that appeared in a string\n",
        "2. Ignore the order in which those words appeared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW6HrMqKzhKw",
        "colab_type": "text"
      },
      "source": [
        "In practice, there are more than two steps required to produce a bag of words model from a raw text input. Lets walk through the process together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_CzF_g1zhKx",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_GJWgiqzhKz",
        "colab_type": "text"
      },
      "source": [
        "There can be several steps here, but the aim of this process is to reduce duplicate and nonsense words from our final corpus. Some of the steps we might need to do include:\n",
        "1. Converting text to lower case\n",
        "2. Removing all non-word characters\n",
        "3. Removing all punctuations\n",
        "4. Removing multiple spaces\n",
        "\n",
        "All these steps are necessary to ensure that only a small set of useful words remain in our corpus. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkuao1Z0zhK1",
        "colab_type": "text"
      },
      "source": [
        "##### Exercise: Have a look at the sentece given below and complete the preprocess function to apply all the above steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDYhL6LgzhK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \" Do Preprocessing on this piece of text! You should   be able to obtain a clean version. Text should always be cleaned\"\n",
        "\n",
        "## Write your code here\n",
        "def preprocess(text):\n",
        "    return ''\n",
        "\n",
        "preprocessed_text = preprocess(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjMbIGlS9-3Z",
        "colab_type": "text"
      },
      "source": [
        "##### Example Answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXy5O_pq96eG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1OC__HtzhK6",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Tokenization "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pug64n8bzhK8",
        "colab_type": "text"
      },
      "source": [
        "This is the process of breaking text into tokens/words. In this simple example we're going to simply break the text around the whitespace (spaces). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy4eSD0YzhK9",
        "colab_type": "text"
      },
      "source": [
        "##### Exercise: First try to create your own tokenization function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWiB1GgyzhK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "    # You must return a list of tokens, you can use the string.split() function\n",
        "    return []\n",
        "\n",
        "tokens = tokenize(preprocessed_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnxCspl7-eGb",
        "colab_type": "text"
      },
      "source": [
        "##### Example Answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH-ovDH4-cJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN_XxEnU2i1P",
        "colab_type": "text"
      },
      "source": [
        "#### 3. Removing Stopwords\n",
        "\n",
        "We need to remove the functional words like 'is', 'and', 'the', 'are' etc as they give no information about the text and increase clutter in the analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGbrtc-N3F78",
        "colab_type": "code",
        "outputId": "161ed387-06e7-49db-d725-b1474b5ca90d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        " \n",
        "stopWords = set(stopwords.words('english'))\n",
        "print(stopWords)\n",
        "\n",
        "def removeStopwords(tokens, stopWords):\n",
        "  # Write your logic to remove stopwords\n",
        "  return []\n",
        "\n",
        "filtered_tokens = removeStopwords(tokens, stopWords)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "{\"needn't\", 'such', 'will', 'theirs', \"shan't\", 'doesn', 'and', \"don't\", \"hasn't\", \"you've\", 'weren', 'had', 'can', 'haven', 'is', \"doesn't\", 'of', \"haven't\", 'herself', 't', 'not', \"wouldn't\", 'here', 'no', 'until', 'she', 'm', \"didn't\", 'under', 'down', 'ma', \"it's\", 'hadn', \"she's\", 'so', 'below', 'more', 'why', 'yourself', \"aren't\", 'he', 'have', 'for', 'has', 'in', 've', 'during', 'few', 'was', 'having', 'me', 'who', 'hers', 'on', 'further', 'into', 'as', 'been', \"should've\", \"weren't\", 'same', \"mustn't\", 'about', 'from', 'to', 'how', \"mightn't\", 'it', 'yours', 'don', \"couldn't\", 'that', 'this', 'once', 'than', 'nor', 'a', 'her', \"isn't\", 'just', 'against', 'while', 'are', 'y', 'aren', 'mustn', 'your', 'at', 'themselves', 'won', 'what', 'should', 'again', 'shouldn', 'does', \"hadn't\", 'you', 'wasn', 'were', 'shan', 'where', 'these', 'the', 'his', 'above', 'hasn', 'doing', 'through', 'then', 'mightn', 'each', \"wasn't\", 'off', 'between', 'their', 'isn', 'but', 'o', \"won't\", 'll', 'i', 'or', 'if', \"you're\", 'they', 'which', 're', 'ain', 'be', 'am', 'ourselves', 'by', 'out', 's', 'very', \"shouldn't\", 'we', 'itself', 'after', 'when', 'yourselves', 'over', 'him', 'them', 'being', 'any', 'own', 'all', \"that'll\", 'do', 'd', 'myself', 'both', 'needn', 'its', 'our', 'an', 'before', 'now', 'up', 'too', 'couldn', 'ours', 'didn', 'my', 'those', \"you'll\", 'only', 'whom', 'other', \"you'd\", 'wouldn', 'himself', 'did', 'some', 'because', 'there', 'most', 'with'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d49e32e256c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoveStopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUDPiLdCzhLF",
        "colab_type": "text"
      },
      "source": [
        "#### 4. Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbl_XY40zhLG",
        "colab_type": "text"
      },
      "source": [
        "In many European languages (e.g English, German, Spanish), words can take on several forms. This process of converting from root form to other ones is called inflection. For example, in English this is used to signal different cases, different actor(s) and differnt tenses. E.g. `play` can become `plays`, `playing`, `played` etc. \n",
        "\n",
        "In most of the cases we'd want to treat all of these form in a similar manner, hence it might be useful to convert them to their root form.\n",
        "\n",
        "Lemmatization is also used for the same purpose but it is more accurate than stemming. Example: If the word ‘go’ is in its past tense ‘went’ in the given sentence, Stemming cant arrive at 'go' from 'went'. But Lemmatization can. Stemming is a rule based technique where Lemmatization is a dictionary based technique. Therefore, Lemmatization is more accurate on the other hand it is slow in processing as it has to look up the dictionary. The base form of a word in this case is called Lemma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dAP4sSlzhLI",
        "colab_type": "text"
      },
      "source": [
        "Since, it is pretty non-trivial to implement stemming by yourself, we'll be using a well known library called `nltk`. Nltk is a multipurpose NLP package that we'll use at several points during this minicourse.\n",
        "\n",
        "\n",
        "A detailed dive into all the techniques we're using has been omitted to keep it short. If you want to go deeper into how stemming works, you can check out here: https://www.geeksforgeeks.org/python-stemming-words-with-nltk/\n",
        "\n",
        "##### Exercise: Complete the function below using nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSaJpftTzhLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "def stem(tokens):\n",
        "    # Use stemming from nltk here\n",
        "    return tokens\n",
        "\n",
        "stemmed_tokens = stem(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2-J-sF9AFHs",
        "colab_type": "text"
      },
      "source": [
        "##### Example Answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcXgiwsrAKo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RYl1DVxzhLR",
        "colab_type": "text"
      },
      "source": [
        "Usually you'll always be using libraries for all the above tasks, but it's alyways a good idea to dirty your hands in the beginning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JJAqBjTzhLU",
        "colab_type": "text"
      },
      "source": [
        "### Count Vectors "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb-LjOqczhLW",
        "colab_type": "text"
      },
      "source": [
        "Everything we've been doing so far has been preparing the text because we want to count the words in the text properly. Now that you know how things work let's look at the count matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cC3UllNzhLY",
        "colab_type": "text"
      },
      "source": [
        "For this task you can use both `gensim` as well as `sklearn`.\n",
        "\n",
        "`gensim` is faster but `sklearn` is easier and has better integration for downstream tasks so we're going to work with that. Here's the example of `CountVectorizer` from sklearn's page:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVoxcqw1zhLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "     'This is the first document.',\n",
        "     'This document is the second document.',\n",
        "     'And this is the third one.',\n",
        "     'Is this the first document?',\n",
        " ]\n",
        " \n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_MpIYFOzhLk",
        "colab_type": "text"
      },
      "source": [
        "Here `X` will contain the counts of each of the words present in the text. Lets take a look at the words in our corpus, and the bag of words representations of each snippet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-BgDOq1zhLm",
        "colab_type": "code",
        "outputId": "04272918-3d9c-41c8-b678-e31df12f5e74",
        "colab": {}
      },
      "source": [
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkBv_vsMzhLq",
        "colab_type": "code",
        "outputId": "bf862616-7a71-4df1-ae05-4f2ee75a9db5",
        "colab": {}
      },
      "source": [
        "print(X.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeCP1zvIzhLw",
        "colab_type": "text"
      },
      "source": [
        "You can change a few parameters to make sure all the irrelevant stop words like (and, or, is, the etc.) are removed. You can also control the max number of words that should be considered. Moreoever tokenizers, stemmers, lemmatizers and other parts of the pipeline can also be provided. Have a look at the documentation to play around with the parameters: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j26AwHxzhLy",
        "colab_type": "text"
      },
      "source": [
        "### tf-idf vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f33BfMIuzhL0",
        "colab_type": "text"
      },
      "source": [
        "tf-idf is an improvement over the count vectors because it takes into account the frequency of a word in a given corpus. \n",
        "\n",
        "Suppose you are working with several documents related to medicine. There'll be a lot of domain specific words that'll occur in most of the documents and are not predictive of anything. For example 'hospital', 'doctor' and 'drugs' might all normally convey significant meaning, but may have little value in this context.\n",
        "\n",
        "We need some way to penalize counts of such words and that's where tf-idf comes in.\n",
        "\n",
        "**The basic idea is:** take the word counts and divide it by the number of different documents that the word occurs in (sometimes with a log of that)\n",
        "\n",
        "Again, as we don't have the time to go into detail on this topic, you can find a more detailed discussion here: https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgsuauQozhL1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "sklearn provides a vectorizer similar to CountVectorizer to directly obtain tfidf-vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNE0TqoPzhL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "     'This is the first document.',\n",
        "     'This document is the second document.',\n",
        "     'And this is the third one.',\n",
        "     'Is this the first document?',\n",
        " ]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEcxV_BAzhL7",
        "colab_type": "code",
        "outputId": "6ce497af-39a2-4395-9639-f4a158b37cf0",
        "colab": {}
      },
      "source": [
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riqk6BgNzhMD",
        "colab_type": "code",
        "outputId": "058af113-75f6-4135-9ea0-b44af0991b49",
        "colab": {}
      },
      "source": [
        "print(X.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB2kWo28zhMN",
        "colab_type": "text"
      },
      "source": [
        "### Using BoW models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjmE7Nv1zhMS",
        "colab_type": "text"
      },
      "source": [
        "Once you convert your text into bow representation (counts or tf-idf) and you have a label corresponding to each of them, you can train any classifier or regression model on top of that. This simple technique of counting words helps solve a lot of task for which you have any labelled data.\n",
        "\n",
        "However, for most problems in the world you do not have any labelled data and you do need to solve the problem upto sufficient extent. We'll look in the later notebooks as to how you can do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFaKZOH8EKc4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u37a00ZGELj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}