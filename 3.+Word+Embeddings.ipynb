{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "3. Word Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCNZBCccidfl",
        "colab_type": "text"
      },
      "source": [
        "# Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-js6XwD48EWD",
        "colab_type": "text"
      },
      "source": [
        "## Quick Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riegjqcz8MQ0",
        "colab_type": "text"
      },
      "source": [
        "We're going to be using a library called SpaCy later in this notebook. To make things easier we're quickly going to download an install the model now.\n",
        "\n",
        "If you're running on local run both steps one after another.\n",
        "\n",
        "For colab:\n",
        "\n",
        "All you need to do is run the following cell. Then, when it's finished running, restart the runtime above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVWLBmu88YPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python -m spacy download en_core_web_md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrGdE46_MhvL",
        "colab_type": "text"
      },
      "source": [
        "Then once you've done the above steps, run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hdv18vR1MtM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_md')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFv2-BHMsgOm",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzwFOEq_idfp",
        "colab_type": "text"
      },
      "source": [
        "In the first notebook in this series, we looked at different ways of representing text. We showed that a string could be represented as an array of words and their frequencies and that this was a useful way of analysing text.\n",
        "\n",
        "Whilst our Bag of Word (BoW) models were increadibly quick and a useful tool for getting started, the trade-off for this is that they fail to capture a lot of information within our text.\n",
        "\n",
        "Pause here and have a think about how we created the bag of words models and see if you can work out what information we are missing.\n",
        "  \n",
        "BoW models are created by assigning dummy variables to indicate the frequence of a word's occurance in a text. Imagine the following creating a BoW model for the following word list:\n",
        "\n",
        "`W = ['great', 'ghost', 'good', 'ghoul', 'grenade', 'gave']`\n",
        "\n",
        "You would get something like this:\n",
        "\n",
        "_  | Great | Ghost | Good | Ghoul | Grenade | Gave\n",
        "--- | --- | --- | --- | --- | --- | ---\n",
        " Great| 1 | 0 | 0 | 0 | 0 | 0\n",
        " Ghost | 0 | 1 | 0 | 0 | 0 | 0\n",
        " Good | 0 | 0 | 1 | 0 | 0 | 0\n",
        " Ghoul | 0 | 0 | 0 | 1 | 0 | 0\n",
        " Grenade | 0 | 0 | 0 | 0 | 1 | 0\n",
        "Gave | 0 | 0 | 0 | 0 | 0 | 1\n",
        "\n",
        "If we focus on the top three lines, we can see that the vectors (shown in each row) for good, great and ghost, are all equally similar and dissimilar to each other:\n",
        "\n",
        "_  | Great | Ghost | Good | Ghoul | Grenade | Gave \n",
        "--- | --- | --- | --- | --- | --- | ---\n",
        "**Great**| **1** | **0** | **0** | **0** | **0** | **0**\n",
        " **Ghost** | **0** | **1** | **0** | **0** | **0** | **0**\n",
        " **Good** | **0** | **0** | **1** | **0** | **0** | **0**\n",
        " Ghoul | 0 | 0 | 0 | 1 | 0 | 0\n",
        " Grenade | 0 | 0 | 0 | 0 | 1 | 0\n",
        " Gave | 0 | 0 | 0 | 0 | 0 | 1 | 0\n",
        "\n",
        "To you or me, however, it is readily apparent that 'good' and 'great' are very similar and 'ghost' and 'ghoul' are quite similar, whilst none should be similar with 'gave' or 'grenade'.\n",
        "\n",
        "The second issue that we face is that from this list we have no idea about how the original sentence was constructed. Presuming stop words were removed beforehand, the sentence could have been:\n",
        "\n",
        "`The good ghost gave a grenade to the great ghoul.`\n",
        "\n",
        "It also could have been many other combinations. This becomes especially troublesome when words could have had different meanings in different places. E.g. was it 'good' as in not evil, or 'good' as in skilled at something?\n",
        "\n",
        "In summary, BoW models fail to capture any infomation about:\n",
        "\n",
        "* How similar a word is to another\n",
        "* Contextual meaning of a word\n",
        "      \n",
        "---\n",
        "\n",
        "The solution to this problem is to create word vectors.\n",
        "\n",
        "Word vectors solve these problems as well as being easy-to-use and interpretable. They can be defined as:\n",
        "\n",
        "*A vector comprised of valued numbers where each dimension captures an aspect of the wordâ€™s meaning* more recently this has expanded to include  *semantically similar words have similar vectors.*\n",
        "\n",
        "What makes this approach even better is that by thinking of words as vectors, we will be able to use mathematical operators on them... more on this later.\n",
        "\n",
        "First, lets look at how vectors achieve similarity and context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRrcs_QYidfs",
        "colab_type": "text"
      },
      "source": [
        "## Word Vectors and Similarity\n",
        "\n",
        "Creating word embeddings is a method of vector representation that capture information about the context of a word, not just the word itself. For example, we would expect similar words to have similar vectors. Take the following two sentences:\n",
        "\n",
        "1. Yesterday I **read** a book\n",
        "2. I will **study** that paper\n",
        "\n",
        "Here the words read and study convey the same information and their vector representation should capture this information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTQ3HsxUwnDY",
        "colab_type": "text"
      },
      "source": [
        "We can visualise this easier if we take a larger amount of words, and see how they compare along some imaginary dimensions\n",
        "\n",
        "![Table of words](https://auquan-public-data.s3.ap-south-1.amazonaws.com/Screenshot+2020-03-24+at+13.37.10.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK5K17jH0BPv",
        "colab_type": "text"
      },
      "source": [
        "We can see in this table that certain words have similar vectors - such as those for NFL and NBA, or England and India. This makes sense on a conceptual level too, as these words refer to similar things.\n",
        "\n",
        "Once part of this table is misleading however. We've labelled the dimensions by giving them clearly defined meanings, this isn't real. In real life the vector dimensions don't each code for a tangiable feature that we would understand. The are abstractions. Imagining that they have an real meaning is helpful for building an intuitive understanding of them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA8hc5Kmwlcv",
        "colab_type": "text"
      },
      "source": [
        "So how do we capture this information in practice? On a technical level, word embeddings have the following properties:\n",
        "\n",
        "1. Each word in the vocabulary is represented by a low dimensional vector (~ 300d)\n",
        "2. All words are embedded into the same space\n",
        "3. Similar words have similar vectors\n",
        "(= their vectors are close to each other in the vector space)\n",
        "4. Word embeddings are successfully used for various NLP application\n",
        "\n",
        "Lets make it real with some examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F93gQCLvQLz6",
        "colab_type": "text"
      },
      "source": [
        "We're going to plot the embeddings of all the words in the table and see how they relate. Since the actual embeddings have around 300 dimensions, we'll be reducing them to 2 for visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByZ4OEG6EDq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# a string with all the words we'll be looking at\n",
        "sentence = \"cricket baseball football handball NFL NBA  basketball spectator superbowl stadium rugby england india tennis badminton court squash\"\n",
        "# creating tokens from the string\n",
        "tokens = nlp(sentence)\n",
        "# creating vectors for each of the words\n",
        "vectors = np.vstack([word.vector for word in tokens if word.has_vector])\n",
        "# reducing dimensions using PCA\n",
        "pca = PCA(n_components=2, random_state=0)\n",
        "# get the vectors\n",
        "vecs_transformed = pca.fit_transform(vectors)\n",
        "# get vectors with labels\n",
        "vecs_transformed = np.c_[sentence.split(), vecs_transformed]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTDLE-haHHaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get x and y coordinates\n",
        "x_coords = np.array([float(x[1]) for x in vecs_transformed])\n",
        "y_coords = np.array([float(x[2]) for x in vecs_transformed])\n",
        "# display scatter plot\n",
        "plt.scatter(x_coords, y_coords)\n",
        "\n",
        "# add labels\n",
        "for label, x, y in vecs_transformed:\n",
        "    plt.annotate(label, xy=(float(x), float(y)), xytext=(0, 0), textcoords='offset points')\n",
        "plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
        "plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw0bjWJXy2Lr",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** Find word embeddings and look at their plots for the following words: dogs, cats, sparrow, eagle, chicken, turkey, sheep, cow, crocodile, lion, shark, fish, whale. Do you see any patterns?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52Mg8Wfxy0Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your answer here\n",
        "# Hint: You need to do exactly what we did in our example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tCrfr84w6Ua",
        "colab_type": "text"
      },
      "source": [
        "#### Measuring Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tUz8AQnOH7s",
        "colab_type": "text"
      },
      "source": [
        "Lets take another list of simple words `dog cat bannana`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYi8N997idf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(\"dog cat banana\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
        "\n",
        "# has_vector: whether there is a word vector for that token\n",
        "# token.vector_norm: norm of the vector\n",
        "# token.is_oov: is the token out of vocabulary\n",
        "# You can find more info here: https://spacy.io/api/token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN3rVLc6Q_28",
        "colab_type": "text"
      },
      "source": [
        "If we look at the vector_norm value, it certainly seens that `cat` and `dog` are much more similar to one another than either one is  with `banana`.\n",
        "\n",
        "But what do we mean by similarity? And what does this look like in a mathematical context?\n",
        "\n",
        "First lets actually compute the similarity values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pibU9JYTgWmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "doc = nlp(\"dog cat banana\")\n",
        "\n",
        "n = len(doc)\n",
        "arr = np.zeros((n, n))\n",
        "\n",
        "for i, token1 in enumerate(doc):\n",
        "    for j, token2 in enumerate(doc):\n",
        "        arr[i, j] = token1.similarity(token2)\n",
        "        print('token1: %s , token2: %s, similarity: %s'%( token1.text, token2.text, token1.similarity(token2)) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKZAixGEg9wT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's plot the similarity measures\n",
        "import seaborn as sn\n",
        "\n",
        "sims = pd.DataFrame(arr, index=[x.text for x in doc], columns=[x.text for x in doc])\n",
        "sn.heatmap(sims, annot=True, fmt='g')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0s_K2FAidgL",
        "colab_type": "text"
      },
      "source": [
        "So we appear to have been right. Cat and Dog are quite similar, and banana isn't. But what do these numbers represent?\n",
        "\n",
        "The similarity defined here is actually the cosine similarity between the two vectors. It is given by Euclidean dot product between the two vectors. Mathematically:\n",
        "\n",
        "$$\\text{similarity} = \\cos(\\theta) = {\\mathbf{A} \\cdot \\mathbf{B} \\over \\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{ \\sum\\limits_{i=1}^{n}{A_i  B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}  \\sqrt{\\sum\\limits_{i=1}^{n}{B_i^2}} }$$\n",
        "\n",
        "In simple terms: **sum of elementwise product of normalized vectors**. Here are some properties of this measure:\n",
        "1. The value can vary between -1 and 1\n",
        "2. cosine distance of a vector with itself is 1 (which makes sense as a vector is the most similar to itself)\n",
        "3. If two vectors are in the same direction, their cosine distance > 0 and < 0 if they are in opposite directions\n",
        "4. If the cosine distance is zero then the vectors are called perpendicular to each other.\n",
        "\n",
        "You can read more about that here: https://en.wikipedia.org/wiki/Cosine_similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcCn9Npq0OlW",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** Try to find the similarity between all pairs of words in the sentence: `cricket baseball football handball NFL NBA`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBK7L5IY0Mfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your answer here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6BZZV8qPmuz",
        "colab_type": "text"
      },
      "source": [
        "#### Hint:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnQWzWtS007Z",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "1. Convert to spacy doc\n",
        "2. Iterate over all the tokens in a nested for loop\n",
        "3. Use `token1.similarity(token2)` to get the similarity measure, just like we did earlier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9NO5YndxKaS",
        "colab_type": "text"
      },
      "source": [
        "## Capturing Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt64izYoidfr",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will look at how vector representations gain their contextual understanding about a word. \n",
        "\n",
        "In the previous section we were able to clearly show that similar words have similar vectors. This isn't possible for the context component. Instead we're going to tell you how the vectors are created. Hopefully this will help demonstrate why they must contain contextual information.\n",
        "\n",
        "The core principle behind the design of modern word vector algorithms is **The Distributional Hypothesis**. This has two main ideas:\n",
        "* Words that occur in the same contexts tend to have similar\n",
        "meanings\n",
        "(Harris, 1954)\n",
        "* â€œYou shall know a word by the company it keepsâ€ (Firth, 1957)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqNc3OWS6jeE",
        "colab_type": "text"
      },
      "source": [
        "We're going to look a model called word2vec as our demonstration. (Newer models build upon this foundation by adding more complicated statistics).\n",
        "\n",
        "Word2Vec uses the context/neighbors of a word to compute the embedding for each of the words. There are two approaches that've been used in the original paper. Without going into much detail let's discuss the intuition behind each one:\n",
        "\n",
        "1. CBOW: Continuous Bag of words\n",
        "2. Skip Grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE76GwX06oYH",
        "colab_type": "text"
      },
      "source": [
        "**CBOW**\n",
        "\n",
        "Suppose you have large amount of text. You're reading it with some window size of say 5 words. At step in moving this window you try to predict the middle word using the neighbors.\n",
        "\n",
        "For example, let's look at the sentence: \n",
        "\n",
        "`Buffaloes are one of the strongest and most dangerous animals in all of Africa`\n",
        "\n",
        "For a window size of 5, you'll be initially reading (after removing the stop words): `Buffaloes one strongest dangerous animals`.\n",
        "In CBOW you'll try to predict the vector for the middle word (in this case `strongest`) using vectors of all other words in the window. Once you've made your guess, you then learn from this by making updates to all of the other vectors\n",
        "\n",
        "In the next step you'll consider: `one strongest dangerous animals Africa` and try to predict `dangerous` using all the other words "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjmHTC9d6zIv",
        "colab_type": "text"
      },
      "source": [
        "**Skip Grams**\n",
        "\n",
        "The process is quite similar to the CBOW but in this case we try to predict the neighbors using the central word in each iteration of the moving window.\n",
        "\n",
        "So for our original words (`Buffaloes one strongest dangerous animals`), we would drop everything apart from strongest and try and predict the outer words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSaiuRogidf1",
        "colab_type": "text"
      },
      "source": [
        "## Creating Word Vectors in Practice\n",
        "To create word embeddings we can continue to use use either Spacy or Gensim. These library's incorporate models we'll describe below. We're going to continue using SpaCy as it's usefull for the next notebook. By default Spacy will create Word2Vec embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lks7XG4HqbyR",
        "colab_type": "text"
      },
      "source": [
        "**Different Word Vector Models:**\n",
        "\n",
        "The most commonly used word embeddings are: **word2vec** and **GloVe**, both used extensively and giving similar results. \n",
        "\n",
        "`Word2Vec` was made by Thomas Mikov at Google in 2013 and introduced the use of CBoW and Skip Grams. It's considered to produce high quality embeddings (vector representations) and to do so reasonably quickly.\n",
        "\n",
        "`GloVe` is an extension on `Word2Vec` that introduces global statistics in the langauge modelling process. This can create much richer embeddings.\n",
        "\n",
        "There are several other noteworthy libraries that you can experiment with. `Bert` and `Elmo` are both recent advances that create different embeddings for different contexts.\n",
        "\n",
        "We're not going to look into how they each work, but rather, we're going to focus on how we can make use of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Er-Dh8UidgC",
        "colab_type": "text"
      },
      "source": [
        "For this notebook we're going to look at using Word2Vec, but you're encouraged to import the other libraries and explore any differences you may find."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5LbDG0nUINz",
        "colab_type": "text"
      },
      "source": [
        "Let's look at actual normalized vectors first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6Ntq_A3UQKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the vector and normalize\n",
        "word = 'man'\n",
        "\n",
        "def getVec(word):\n",
        "  vec = nlp(word).vector / nlp(word).vector_norm\n",
        "  return vec\n",
        "\n",
        "vec = getVec(word)\n",
        "print(vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHuL-lIGXZ1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get similarity between the vectors\n",
        "import numpy as np\n",
        "\n",
        "# directly using spacy\n",
        "print(nlp('car').similarity(nlp('truck')))\n",
        "\n",
        "# reproducing the results yourself\n",
        "print(np.dot( getVec('car'), getVec('truck')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxNYgk64Xy6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get difference between two vectors\n",
        "getVec('read') - getVec('study')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1fSqRCRUFWU",
        "colab_type": "text"
      },
      "source": [
        "**Now answer the following questions**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTmdIRUwYUKt",
        "colab_type": "text"
      },
      "source": [
        "#### **Exercise 1:** What is `man - woman`?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYvVNHJzYhzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## get the normalized vectors and find the difference"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jk3ogEyP_hB",
        "colab_type": "text"
      },
      "source": [
        "##### *Example Solution:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9Iz_BjFQFB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diff1 = getVec('man') - getVec('woman')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yESd_NDVYXi5",
        "colab_type": "text"
      },
      "source": [
        "#### **Exercise 2:** What is `king - queen`?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FloLsTf7Ym8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## same as above"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CHfokkX8QOvY"
      },
      "source": [
        "##### *Example Solution:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gmxii6h-QOvb",
        "colab": {}
      },
      "source": [
        "diff2 = getVec('king') - getVec('queen')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUnjYwHlYZj7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### **Exercise 3:** Are 1 and 2 close?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeAOwE8sYqe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## get the cosine distance between 1 and 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1GKh3lrGQPzj"
      },
      "source": [
        "##### *Example Solution:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XphhXyAdQPzk",
        "colab": {}
      },
      "source": [
        "np.dot( diff1, diff2) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3Q3uFO2Ya7a",
        "colab_type": "text"
      },
      "source": [
        "#### **Exercise 4:**  What are the similarities between `Japan and Tokyo`, between `India and New Delhi`? Are they similar?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZYr9n8kYWiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## use everything you've learnt so far"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xTEzvBsfQQux"
      },
      "source": [
        "##### *Example Solution:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "abuGLB6oQQuy",
        "colab": {}
      },
      "source": [
        "nlp('Japan').similarity(nlp('Tokyo')), nlp('India').similarity(nlp('New Delhi')) "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}